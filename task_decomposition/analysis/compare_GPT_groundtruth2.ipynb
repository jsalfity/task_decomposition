{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from typing import Union\n",
    "\n",
    "from task_decomposition.utils.plotting import visualize_trajectory_decompositions\n",
    "from task_decomposition.paths import DATA_GT_TXT_PATH, GPT_OUTPUT_PATH\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "ROUND_DIGITS = 4\n",
    "\n",
    "START_IDX = 0\n",
    "END_IDX = 1\n",
    "SUBTASK_NAME_IDX = 2\n",
    "\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subtask_from_groundtruth_file(filepath: str) -> list:\n",
    "    '''\n",
    "    This function extracts the subtask from the groundtruth file\n",
    "    The groundtruth file is a txt file with the following format:\n",
    "    step\tsubtask\tstage\n",
    "    0\tAlign manipulator height with Door\t0\n",
    "    1\tAlign manipulator height with Door\t0\n",
    "    2\tAlign manipulator height with Door\t0\n",
    "    3\tAlign manipulator height with Door\t0\n",
    "    4\tAlign manipulator height with Door\t0\n",
    "    5\tAlign manipulator height with Door\t0\n",
    "    6\tAlign manipulator height with Door\t0\n",
    "    7\tGet closer to Door\t1\n",
    "    8\tGet closer to Door\t1\n",
    "    9\tGet closer to Door\t1\n",
    "    10\tGet closer to Door\t1\n",
    "    11\tGet closer to Door\t1\n",
    "    12\tGet closer to Door\t1\n",
    "    13\tGet closer to Door\t1\n",
    "    14\tGet closer to Door\t1\n",
    "    15\tGet closer to Door\t1\n",
    "    16\tGet closer to Door\t1\n",
    "    17\tGet closer to Door\t1\n",
    "    18\tGet closer to Door\t1\n",
    "    ...\n",
    "\n",
    "    The function returns a list of subtasks that specifies the start and end step of each subtask\n",
    "    [(<start_step>, <end_step>, <subtask_name>), ...]\n",
    "    '''\n",
    "    _step_idx = 0\n",
    "    _subtask_idx = 1\n",
    "    subtask_decomposition = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        # read and remove the header\n",
    "        lines = f.readlines()\n",
    "        lines = lines[1:] if lines[0].startswith('step') else lines\n",
    "\n",
    "        # set initial values\n",
    "        current_subtask = lines[0].split('\\t')[_subtask_idx]\n",
    "        start_step = int(lines[0].split('\\t')[_step_idx])\n",
    "\n",
    "        for idx, line in enumerate(lines):\n",
    "            subtask = line.split('\\t')[_subtask_idx]\n",
    "            if subtask != current_subtask:\n",
    "                # log values\n",
    "                end_step = int(line.split('\\t')[_step_idx])-1\n",
    "                subtask_decomposition.append((start_step, end_step, current_subtask))\n",
    "\n",
    "                # reset values\n",
    "                start_step = int(line.split('\\t')[_step_idx])\n",
    "                current_subtask = subtask\n",
    "            elif idx == len(lines) - 1:\n",
    "                end_step = int(line.split('\\t')[_step_idx])\n",
    "                subtask_decomposition.append((start_step, end_step, current_subtask))\n",
    "\n",
    "            # is there a corner case here where we are missing the last element should there be a change?\n",
    "\n",
    "    return subtask_decomposition\n",
    "\n",
    "def extract_subtask_from_gpt_output_file(filepath: str) -> list:\n",
    "    '''\n",
    "    This function extracts the subtask from the gpt output file.\n",
    "    The gpt output file is a json, with the field \"response\" containing the output of the gpt model.\n",
    "    '''\n",
    "    # read the json file and load as a dictionary\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    response = data[\"response\"]\n",
    "    start = response.find('subtask_decomposition = [') + len('subtask_decomposition = [')\n",
    "    end = response.find(']', start)\n",
    "    list_str = response[start:end]\n",
    "\n",
    "    # Converting string representation of list to actual Python list\n",
    "    subtask_decomposition = ast.literal_eval('[' + list_str + ']')\n",
    "    return subtask_decomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(subtask_A: tuple, subtask_B: tuple) -> bool:\n",
    "    '''\n",
    "    This function checks if two subtasks intersect.\n",
    "    '''\n",
    "    a1, a2 = subtask_A[START_IDX], subtask_A[END_IDX]\n",
    "    b1, b2 = subtask_B[START_IDX], subtask_B[END_IDX]\n",
    "\n",
    "    return a1 <= b2 and b1 <= a2\n",
    "\n",
    "def get_IOU(subtask_A: tuple, subtask_B: tuple) -> float:\n",
    "    a1, a2 = subtask_A[START_IDX], subtask_A[END_IDX]\n",
    "    b1, b2 = subtask_B[START_IDX], subtask_B[END_IDX]\n",
    "\n",
    "    # Calculate the intersection\n",
    "    intersection = max(0, min(a2, b2) - max(a1, b1))\n",
    "\n",
    "    # Calculate the union\n",
    "    union = max(a2, b2) - min(a1, b1)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if union == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate the IoU\n",
    "    iou = intersection / union\n",
    "\n",
    "    return round(iou, ROUND_DIGITS)\n",
    "\n",
    "def bert_encode(text):\n",
    "    \"\"\"\n",
    "    Encode the text using BERT\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)[0].detach().numpy()\n",
    "\n",
    "def compare_description_similarity(A: str, B:str) -> float:\n",
    "    \"\"\"\n",
    "    Compare the similarity between two descriptions using BERT embeddings\n",
    "    \"\"\"\n",
    "    embedding1 = bert_encode(A)\n",
    "    embedding2 = bert_encode(B)\n",
    "    similarity = 1 - cosine(embedding1, embedding2)\n",
    "    return round(similarity, ROUND_DIGITS)\n",
    "\n",
    "# def find_best_matches(subtask: tuple, subtask_decomp_B: list) -> Union[list, list]:\n",
    "    \"\"\"\n",
    "    Find the best matches above a certain semantic threshold\n",
    "    \"\"\"\n",
    "    THRESHOLD = 0.7\n",
    "    best_matches = []\n",
    "    BERT_scores = []\n",
    "    for s in subtask_decomp_B:\n",
    "        semantic_score = compare_description_similarity(subtask[2], s[2])\n",
    "        if semantic_score > THRESHOLD:\n",
    "            best_matches.append(s)\n",
    "            BERT_scores.append(semantic_score)\n",
    "\n",
    "    return best_matches, BERT_scores\n",
    "\n",
    "def subtask_similarity(subtask_decomp_A: list, subtask_decomp_B: list) -> None:\n",
    "    '''\n",
    "    This function calculates the similarity between two subtasks.\n",
    "    The metric follows the transitive property as sim(A, B) = sim(B, A)\n",
    "    '''\n",
    "    weight_temporal = 0.5\n",
    "    weight_semantic = 0.5\n",
    "\n",
    "    N = subtask_decomp_A[-1][END_IDX] + 1 # +1 because index starts at 0\n",
    "    score = 0\n",
    "    for subtask in subtask_decomp_A:\n",
    "\n",
    "        subtask_temporal_relative_weight = (subtask[END_IDX]- subtask[START_IDX]+1)/ N\n",
    "\n",
    "        # find subtasks that intersect temporally\n",
    "        intersect_subtasks = [s for s in subtask_decomp_B if intersection(subtask, s)]\n",
    "\n",
    "        # find semantic similarity with all intersecting subtasks\n",
    "        # at the same time, combine the subtasks that are sequential\n",
    "        combined_intersect_subtasks = []\n",
    "        BERT_scores = []\n",
    "        for intersect_subtask in intersect_subtasks:\n",
    "            BERT_scores.append(compare_description_similarity(subtask[SUBTASK_NAME_IDX], intersect_subtask[SUBTASK_NAME_IDX]))\n",
    "            if len(combined_intersect_subtasks) == 0:\n",
    "                combined_intersect_subtasks.append(intersect_subtask)\n",
    "            else:\n",
    "                prev_subtask = combined_intersect_subtasks[-1]\n",
    "                if prev_subtask[END_IDX] == intersect_subtask[START_IDX] - 1:\n",
    "                    combined_intersect_subtasks[-1] = (prev_subtask[START_IDX], intersect_subtask[END_IDX], prev_subtask[SUBTASK_NAME_IDX])\n",
    "                else:\n",
    "                    combined_intersect_subtasks.append(intersect_subtask)\n",
    "\n",
    "\n",
    "        # Need to combine sequential subtasks\n",
    "        IOUs = np.sum([get_IOU(subtask, s) for s in combined_intersect_subtasks])\n",
    "        relative_IOUs = IOUs * subtask_temporal_relative_weight\n",
    "\n",
    "        mean_BERT_score = np.mean(BERT_scores)\n",
    "        relative_BERT_score = mean_BERT_score * subtask_temporal_relative_weight\n",
    "\n",
    "        if False:\n",
    "            print(f\"Subtask: {subtask[SUBTASK_NAME_IDX]}\")\n",
    "            print(f\"(start, end): ({subtask[START_IDX]}, {subtask[END_IDX]})\")\n",
    "            print(f\"Intersecting subtasks: {intersect_subtasks}\")\n",
    "            print(f\"Combined intersecting subtasks: {combined_intersect_subtasks}\")\n",
    "            print(f\"Relative IOU: {relative_IOUs}\")\n",
    "            print(f\"IOU: {IOUs}\")\n",
    "            print(f\"BERT scores: {BERT_scores}\")\n",
    "            print(N)\n",
    "            print(\" \")\n",
    "            score += weight_temporal*relative_IOUs + weight_semantic*relative_BERT_score\n",
    "        score += weight_temporal*relative_IOUs + weight_semantic*relative_BERT_score\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 12, 'Move to cube'), (13, 14, 'Grasp Cube'), (15, 49, 'Lift Cube')]\n",
      "[(0, 9, 'move end effector towards cube'), (10, 12, 'adjust gripper above cube'), (13, 14, 'close gripper to grasp cube'), (15, 21, 'lift cube upwards'), (22, 49, 'hold cube in lifted position')]\n",
      " \n",
      "  \n",
      "Score1: 0.8524385000000001\n",
      "  \n",
      "Score2: 0.669016\n"
     ]
    }
   ],
   "source": [
    "filepath = DATA_GT_TXT_PATH + \"/Lift_20240213-110117_5_gt.txt\"\n",
    "subtask_decomposition = extract_subtask_from_groundtruth_file(filepath)\n",
    "print(subtask_decomposition)\n",
    "filepath = GPT_OUTPUT_PATH + \"/Lift_20240213-110117_5.json\"\n",
    "gpt_subtask_decomposition = extract_subtask_from_gpt_output_file(filepath)\n",
    "print(gpt_subtask_decomposition)\n",
    "\n",
    "print(\" \")\n",
    "score1 = subtask_similarity(subtask_decomposition, gpt_subtask_decomposition)\n",
    "print(\"  \")\n",
    "print(f\"Score1: {score1}\")\n",
    "\n",
    "score2 = subtask_similarity(gpt_subtask_decomposition, subtask_decomposition)\n",
    "print(\"  \")\n",
    "print(f\"Score2: {score2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
