{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, Audio\n",
    "\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "import imageio\n",
    "import cv2  # We're using OpenCV to read video\n",
    "import base64\n",
    "import time\n",
    "import openai\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from task_decomposition.paths import DATA_VIDEOS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 frames read.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"../../data/robomimic/tool_hang/tool_hang_annotated.mp4\"\n"
     ]
    }
   ],
   "source": [
    "# video = cv2.VideoCapture(\"../data/stack_video.mov\")\n",
    "# video = cv2.VideoCapture(\"../../data/robomimic/transport/transport_annotated.mp4\")\n",
    "video = cv2.VideoCapture(DATA_VIDEOS_PATH+\"/tool_hang_annotated.mp4\")\n",
    "\n",
    "base64Frames = []\n",
    "while video.isOpened():\n",
    "    success, frame = video.read()\n",
    "    if not success:\n",
    "        break\n",
    "    _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "    base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "\n",
    "video.release()\n",
    "print(len(base64Frames), \"frames read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store frames with step number on them\n",
    "video_writer = imageio.get_writer(\"../../data/robomimic/tool_hang/video_annotated.mp4\", fps=100)\n",
    "for i, base64_frame in enumerate(base64Frames):\n",
    "    # Decode the base64 string\n",
    "    img_data = base64.b64decode(base64_frame.encode(\"utf-8\"))\n",
    "\n",
    "    # Convert binary data to numpy array\n",
    "    nparr = np.frombuffer(img_data, np.uint8)\n",
    "\n",
    "    # Decode image\n",
    "    frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "\n",
    "    # Put timestamp on the frame\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame, f\"frame number: {i}\", (10, 30), font, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Convert the frame back to an Image object\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # pil_img = PILImage.fromarray(frame)\n",
    "\n",
    "    # Update the display handle\n",
    "    # display_handle.update(pil_img)\n",
    "\n",
    "    video_writer.append_data(frame)\n",
    "\n",
    "video_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_handle = display(None, display_id=True)\n",
    "for img in base64Frames:\n",
    "    display_handle.update(Image(data=base64.b64decode(img.encode(\"utf-8\"))))\n",
    "    time.sleep(0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{1.1: {'start_step: 1, 'end_step': 12, subtask: 'Robot approaching the drawer'},\n",
      "1.2: {'start_step: 13, 'end_step': 24, subtask: 'Robot gripping the drawer handle'},\n",
      "2.1: {'start_step: 25, 'end_step': 36, subtask: 'Robot opening the drawer'},\n",
      "2.2: {'start_step: 37, 'end_step': 48, subtask: 'Robot reaching into the drawer'},\n",
      "3.1: {'start_step: 49, 'end_step': 60, subtask: 'Robot gripping an object inside the drawer'},\n",
      "3.2: {'start_step: 61, 'end_step': 72, subtask: 'Robot extracting the object from the drawer'},\n",
      "4.1: {'start_step: 73, 'end_step': 84, subtask: 'Robot maneuvering the object into position'},\n",
      "4.2: {'start_step: 85, 'end_step': 96, subtask: 'Robot placing the object onto a specific location'},\n",
      "5.1: {'start_step: 97, 'end_step': 108, subtask: 'Robot releasing the object'},\n",
      "5.2: {'start_step: 109, 'end_step': 120, subtask: 'Robot retracting from the placed object'},\n",
      "6.1: {'start_step: 121, 'end_step': 132, subtask: 'Robot returning to the initial rest position'}\n",
      "}\n",
      "```\n",
      "\n",
      "Explanation of thoughts:\n",
      "\n",
      "- In the first image step, number 0, the robot is in an idle position, suggesting the beginning of the episode. Hence, the first subtask will begin from step number 1.\n",
      "- The second image showing step number 12 suggests the robot has moved towards the drawer, which tells us the first subtask involves the robot approaching the drawer.\n",
      "- By step number 24 in the third image, the robot appears to have gripped the handle, so the intervening steps involve this action.\n",
      "- The fourth image at step 36 shows the drawer being open, indicating the robot has performed the task of opening the drawer.\n",
      "- Next, in the fifth image at step number 48, the robot looks like it is reaching into the drawer, meaning those steps involved in this action.\n",
      "- The sixth image at step number 60 shows the robot may have gripped an object inside the drawer, so the subtask between the previous and this step is the gripping of the object.\n",
      "- In the seventh image, step number 72 shows the robot in the process of extracting the object, which is another distinct subtask.\n",
      "- At step number 84 in the eighth image, it looks like the robot is maneuvering the object, which suggests this as another subtask.\n",
      "- The following image with step number 96 presumably involves the step where the robot is placing the object onto a specific location.\n",
      "- By step number 108 shown in the tenth image, the task of the robot releasing the object is noted.\n",
      "- Next, step number 120 in the eleventh image likely represents the robot retracting from the position where the object was placed.\n",
      "- Finally, the last image at step number 132 shows the robot returning to its initial position, representing the final subtask.\n",
      "\n",
      "Each subtask was derived based on the changes in the robot's position and actions with respect to the drawer and the object within it from step to step, as shown in the images provided. The steps at which changes in action are visible help delineate the start and end of each subtask.\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"Your task is to ingest the following data and break down what occurred during the robot episode into hierarchical granular subtasks. Each subtask should be a sequential step that occurred during the robot episode. The simulation step is associated with the frame I'm providing to you. You should identify the start step and end step of each subtask as its written in red on the video. Create as many granular subtasks as you see in the data.\n",
    "\n",
    "Use chain of thought to break down the data into subtasks.\n",
    "\n",
    "Report the list of subtasks as a dictionary with at the beginning of your response:\n",
    "```subtask_decomposition = [\n",
    "{1.1: {'start_step: 1, 'end_step': 10, subtask: '<low level primitive>'},\n",
    "1.2: {'start_step: 11, 'end_step': 20, subtask: '<low level primitive>'},\n",
    "2.1: {'start_step: 21, 'end_step': 30, subtask: '<low level primitive>'},\n",
    "...\n",
    "}]\n",
    "```\n",
    "Then explain your thoughts.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_MESSAGES = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [PROMPT,\n",
    "            *map(lambda x: {\"image\": x, \"resize\": 480}, base64Frames[0::12]),\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "params = {\n",
    "    \"model\": \"gpt-4-vision-preview\",\n",
    "    \"messages\": PROMPT_MESSAGES,\n",
    "    \"api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "    \"headers\": {\"Openai-Version\": \"2020-11-07\"},\n",
    "    \"max_tokens\": 1000,\n",
    "}\n",
    "\n",
    "result = openai.ChatCompletion.create(**params)\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-8NloBLqMA2KyslwKusXkl5wucRpYH at 0x7fd3cada17c0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_details\": {\n",
       "        \"stop\": \"<|fim_suffix|>\",\n",
       "        \"type\": \"stop\"\n",
       "      },\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"```\\n{1.1: {'start_step: 1, 'end_step': 12, subtask: 'Robot approaching the drawer'},\\n1.2: {'start_step: 13, 'end_step': 24, subtask: 'Robot gripping the drawer handle'},\\n2.1: {'start_step: 25, 'end_step': 36, subtask: 'Robot opening the drawer'},\\n2.2: {'start_step: 37, 'end_step': 48, subtask: 'Robot reaching into the drawer'},\\n3.1: {'start_step: 49, 'end_step': 60, subtask: 'Robot gripping an object inside the drawer'},\\n3.2: {'start_step: 61, 'end_step': 72, subtask: 'Robot extracting the object from the drawer'},\\n4.1: {'start_step: 73, 'end_step': 84, subtask: 'Robot maneuvering the object into position'},\\n4.2: {'start_step: 85, 'end_step': 96, subtask: 'Robot placing the object onto a specific location'},\\n5.1: {'start_step: 97, 'end_step': 108, subtask: 'Robot releasing the object'},\\n5.2: {'start_step: 109, 'end_step': 120, subtask: 'Robot retracting from the placed object'},\\n6.1: {'start_step: 121, 'end_step': 132, subtask: 'Robot returning to the initial rest position'}\\n}\\n```\\n\\nExplanation of thoughts:\\n\\n- In the first image step, number 0, the robot is in an idle position, suggesting the beginning of the episode. Hence, the first subtask will begin from step number 1.\\n- The second image showing step number 12 suggests the robot has moved towards the drawer, which tells us the first subtask involves the robot approaching the drawer.\\n- By step number 24 in the third image, the robot appears to have gripped the handle, so the intervening steps involve this action.\\n- The fourth image at step 36 shows the drawer being open, indicating the robot has performed the task of opening the drawer.\\n- Next, in the fifth image at step number 48, the robot looks like it is reaching into the drawer, meaning those steps involved in this action.\\n- The sixth image at step number 60 shows the robot may have gripped an object inside the drawer, so the subtask between the previous and this step is the gripping of the object.\\n- In the seventh image, step number 72 shows the robot in the process of extracting the object, which is another distinct subtask.\\n- At step number 84 in the eighth image, it looks like the robot is maneuvering the object, which suggests this as another subtask.\\n- The following image with step number 96 presumably involves the step where the robot is placing the object onto a specific location.\\n- By step number 108 shown in the tenth image, the task of the robot releasing the object is noted.\\n- Next, step number 120 in the eleventh image likely represents the robot retracting from the position where the object was placed.\\n- Finally, the last image at step number 132 shows the robot returning to its initial position, representing the final subtask.\\n\\nEach subtask was derived based on the changes in the robot's position and actions with respect to the drawer and the object within it from step to step, as shown in the images provided. The steps at which changes in action are visible help delineate the start and end of each subtask.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1700676011,\n",
       "  \"id\": \"chatcmpl-8NloBLqMA2KyslwKusXkl5wucRpYH\",\n",
       "  \"model\": \"gpt-4-1106-vision-preview\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 744,\n",
       "    \"prompt_tokens\": 1239,\n",
       "    \"total_tokens\": 1983\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
